{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiIaJvSdJwQY"
   },
   "source": [
    "### SVHN neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20M1kE59JwQb"
   },
   "source": [
    "#### Import the dataset from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "Oe-f3AJDJwQd",
    "outputId": "583b3fe3-f07a-429b-dc41-9e4fe0141a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CxRNsL_fJwQm",
    "outputId": "ee7f6156-b6c0-4350-da24-af7d7c092130",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 1024) (42000,)\n",
      "Test set (18000, 1024) (18000,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the file as readonly\n",
    "h5f = h5py.File('/content/drive/My Drive/DLCP/Project-1/Data/SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "x_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "x_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1024)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1024)\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "X_val = x_test\n",
    "y_val = y_test\n",
    "\n",
    "print('Training set', x_train.shape, y_train.shape)\n",
    "print('Test set', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "YrL5mONFJwQw",
    "outputId": "51b726ec-8ce9-41c6-99d4-d5d3fb3dbd4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024)\n",
      "(42000,)\n",
      "(18000, 1024)\n",
      "(18000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TxCTEZhvJwQ5",
    "outputId": "aea16695-6e9d-4540-e43a-fd14665d216c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 1024)\n",
      "(18000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVyyx2EWJwRB"
   },
   "source": [
    "#### Define the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQVCEg-oJwRD"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        output = np.dot(self.X, self.W) + self.b\n",
    "        return output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradW = np.dot(self.X.T, nextgrad)\n",
    "        self.gradB = np.sum(nextgrad, axis=0)\n",
    "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ajOLZGDJwRL"
   },
   "source": [
    "#### Define the Rectified Linear Activation Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6cGZiWvJwRN"
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.output = np.maximum(X, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradInput = nextgrad.copy()\n",
    "        self.gradInput[self.output <=0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUqbXqb_JwRY"
   },
   "source": [
    "#### Define the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BsNW9dvJwRb"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-r_hABSOJwRv"
   },
   "source": [
    "#### Define the Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYnqOFbYJwRx"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
    "        loss = np.sum(cross_entropy) / self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()        \n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m), y] -= 1\n",
    "        grad /= self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30c3U72WJwR3"
   },
   "source": [
    "#### Define the container NN class that enables the forward prop and backward propagation of the entire network. Note, how this class enables us to add layers of different types and also correctly pass gradients using the chain rule. Add L2 Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tnqa4-IpJwR5"
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, lossfunc=CrossEntropy(), mode='train'):\n",
    "        self.params = []\n",
    "        self.layers = []\n",
    "        self.loss_func = lossfunc\n",
    "        self.grads = []\n",
    "        self.mode = mode\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            nextgrad, grad = layer.backward(nextgrad)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(out,y)  + ((Lambda / (2 * y.shape[0])) * np.sum([np.sum(w**2) for w in self.params[0][0]]))\n",
    "        nextgrad = self.loss_func.backward(out,y) + ((Lambda/y.shape[0]) * np.sum([np.sum(w) for w in self.params[0][0]]))\n",
    "        grads = self.backward(nextgrad)\n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def predict_scores(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return p\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XY_sGD4JwR_"
   },
   "source": [
    "#### Define the update function (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQRpO8VtJwSB"
   },
   "outputs": [],
   "source": [
    "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = (mu * v[i]) - (learning_rate * g[i])\n",
    "            p[i] += v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLIcUQVXJwSL"
   },
   "source": [
    "#### Define a function which gives us the minibatches (both the datapoint and the corresponding label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLTgFhAiJwSM"
   },
   "outputs": [],
   "source": [
    "def minibatch(X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    for i in range(0, n , minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, :]\n",
    "        y_batch = y[i:i + minibatch_size, ]\n",
    "\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtR4rPZGJwST"
   },
   "source": [
    "#### The traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7b3lFlRWJwSV"
   },
   "outputs": [],
   "source": [
    "def sgd(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9, X_val=None, y_val=None, Lambda=0, verb=True):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "            \n",
    "        # iterate over mini batches\n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "        \n",
    "        # accuracy of model at end of epoch after all mini batch updates\n",
    "        m_train = X_train.shape[0]\n",
    "        m_val = X_val.shape[0]\n",
    "        y_train_pred = []\n",
    "        y_val_pred = []\n",
    "        y_train1 = []\n",
    "        y_vall = []\n",
    "        for ii in range(0, m_train, minibatch_size):\n",
    "            X_tr = X_train[ii:ii + minibatch_size, : ]\n",
    "            y_tr = y_train[ii:ii + minibatch_size,]\n",
    "            y_train1 = np.append(y_train1, y_tr)\n",
    "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
    "\n",
    "        for ii in range(0, m_val, minibatch_size):\n",
    "            X_va = X_val[ii:ii + minibatch_size, : ]\n",
    "            y_va = y_val[ii:ii + minibatch_size,]\n",
    "            y_vall = np.append(y_vall, y_va)\n",
    "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
    "            \n",
    "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
    "        \n",
    "        ## weights\n",
    "        w = np.array(net.params[0][0])\n",
    "        \n",
    "        ## adding regularization to cost\n",
    "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n",
    "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "        \n",
    "        val_loss_epoch.append(mean_val_loss)\n",
    "        if verb:\n",
    "            if i%5 == 0:\n",
    "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n",
    "    return net, [val_acc,  mean_val_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQerMyBCJwSe"
   },
   "source": [
    "#### Checking the accuracy of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJOcHbKVJwSh"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(y_true, y_pred):\n",
    "    count = 0\n",
    "    for i,j in zip(y_true, y_pred):\n",
    "        if int(i)==j:\n",
    "            count +=1\n",
    "    return float(count)/float(len(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwgHc4QoJwSm"
   },
   "source": [
    "#### Invoking all that we have created until now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bxbq3_tZJwSo"
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## input size\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "def train_and_test_loop(iterations, lr, Lambda, verb=True):\n",
    "    ## hyperparameters\n",
    "    iterations = iterations\n",
    "    learning_rate = lr\n",
    "    hidden_nodes = 128\n",
    "    output_nodes = 10\n",
    "\n",
    "    ## define neural net\n",
    "    nn = NN()\n",
    "    nn.add_layer(Linear(input_dim, hidden_nodes))\n",
    "    nn.add_layer(ReLU())\n",
    "    nn.add_layer(Linear(hidden_nodes, output_nodes))\n",
    "\n",
    "    nn, [val_acc, val_loss] = sgd(nn, x_train , y_train, minibatch_size=420, epoch=iterations, learning_rate=learning_rate,\\\n",
    "                      X_val=X_val, y_val=y_val, Lambda=Lambda, verb=verb)\n",
    "    return [val_acc, val_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEfyMV3UJwSu"
   },
   "source": [
    "#### Repeat all the steps given in Babysitting process on SVHN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "b3gBfBFJJwSv",
    "outputId": "f55a89b6-66ff-4cf2-ecb0-ddbdb2e5880a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 2.30254291097064 | Training Accuracy = 0.12335714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12227777777777778"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "Lambda = 0\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYjCr2pe1e6K"
   },
   "source": [
    "Loss and accuracy is reasonable for untrained network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Or3G3TbSJwSx",
    "outputId": "b1e30424-f1a3-41f5-c33d-e69dbefb6e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 3.1446824563871853e+128 | Training Accuracy = 0.09966666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10077777777777777"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 1e3\n",
    "train_and_test_loop(1, lr, Lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBCiZs-M1nQk"
   },
   "source": [
    "Loss went up because of Lambda factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTWcExQwJwS7"
   },
   "outputs": [],
   "source": [
    "x_train_subset = x_train[30:50,]\n",
    "y_train_subset = y_train[30:50,]\n",
    "x_train = x_train_subset\n",
    "y_train = y_train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VjXjhsYnJwTL",
    "outputId": "316e2c6d-5f27-4e95-9928-14b0816ff274"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VW4hF9-0JwTR",
    "outputId": "e03544e2-c84c-484f-bf65-3d5ec696235a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3O18ejVZTqMs"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2h1ym8wMZcNR"
   },
   "source": [
    "Overfit very small portion of the training data\n",
    "So, set a small learning rate and turn regularization off\n",
    "\n",
    "In the code below:\n",
    "- Take the first 20 examples from SVHN\n",
    "- turn off regularization(reg=0.0)\n",
    "- use simple vanilla 'sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3607
    },
    "colab_type": "code",
    "id": "CLWnqE2uJwTV",
    "outputId": "18ec27c8-4c92-4ab4-ec74-a1e7834b0b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000: Loss = 2.303775449383137 | Training Accuracy = 0.1\n",
      "Epoch 5/1000: Loss = 2.286312810648771 | Training Accuracy = 0.2\n",
      "Epoch 10/1000: Loss = 2.2674454469013563 | Training Accuracy = 0.2\n",
      "Epoch 15/1000: Loss = 2.243303526792781 | Training Accuracy = 0.2\n",
      "Epoch 20/1000: Loss = 2.2134474609280255 | Training Accuracy = 0.2\n",
      "Epoch 25/1000: Loss = 2.181553090815501 | Training Accuracy = 0.2\n",
      "Epoch 30/1000: Loss = 2.154668509191717 | Training Accuracy = 0.2\n",
      "Epoch 35/1000: Loss = 2.136374610169531 | Training Accuracy = 0.2\n",
      "Epoch 40/1000: Loss = 2.124409072670691 | Training Accuracy = 0.2\n",
      "Epoch 45/1000: Loss = 2.1157275418460517 | Training Accuracy = 0.2\n",
      "Epoch 50/1000: Loss = 2.1087511606039735 | Training Accuracy = 0.2\n",
      "Epoch 55/1000: Loss = 2.10286769490577 | Training Accuracy = 0.2\n",
      "Epoch 60/1000: Loss = 2.09775660446433 | Training Accuracy = 0.2\n",
      "Epoch 65/1000: Loss = 2.093221132563362 | Training Accuracy = 0.2\n",
      "Epoch 70/1000: Loss = 2.0891039379573835 | Training Accuracy = 0.2\n",
      "Epoch 75/1000: Loss = 2.0852983753293564 | Training Accuracy = 0.2\n",
      "Epoch 80/1000: Loss = 2.081699699815644 | Training Accuracy = 0.2\n",
      "Epoch 85/1000: Loss = 2.078267753997109 | Training Accuracy = 0.2\n",
      "Epoch 90/1000: Loss = 2.0749666705291414 | Training Accuracy = 0.2\n",
      "Epoch 95/1000: Loss = 2.0717411773223886 | Training Accuracy = 0.2\n",
      "Epoch 100/1000: Loss = 2.068572968803548 | Training Accuracy = 0.2\n",
      "Epoch 105/1000: Loss = 2.0654210480112494 | Training Accuracy = 0.2\n",
      "Epoch 110/1000: Loss = 2.06227797237464 | Training Accuracy = 0.2\n",
      "Epoch 115/1000: Loss = 2.0591122431099085 | Training Accuracy = 0.2\n",
      "Epoch 120/1000: Loss = 2.0559128683336807 | Training Accuracy = 0.2\n",
      "Epoch 125/1000: Loss = 2.0526497925523226 | Training Accuracy = 0.2\n",
      "Epoch 130/1000: Loss = 2.049315567546531 | Training Accuracy = 0.2\n",
      "Epoch 135/1000: Loss = 2.045897397597839 | Training Accuracy = 0.2\n",
      "Epoch 140/1000: Loss = 2.0423727771959364 | Training Accuracy = 0.2\n",
      "Epoch 145/1000: Loss = 2.0387309823414648 | Training Accuracy = 0.2\n",
      "Epoch 150/1000: Loss = 2.034974469126678 | Training Accuracy = 0.2\n",
      "Epoch 155/1000: Loss = 2.0310680933084 | Training Accuracy = 0.2\n",
      "Epoch 160/1000: Loss = 2.026999070534292 | Training Accuracy = 0.2\n",
      "Epoch 165/1000: Loss = 2.0227668061700443 | Training Accuracy = 0.2\n",
      "Epoch 170/1000: Loss = 2.01834920162832 | Training Accuracy = 0.2\n",
      "Epoch 175/1000: Loss = 2.0137115960249883 | Training Accuracy = 0.2\n",
      "Epoch 180/1000: Loss = 2.0088777897742043 | Training Accuracy = 0.2\n",
      "Epoch 185/1000: Loss = 2.003774347285254 | Training Accuracy = 0.2\n",
      "Epoch 190/1000: Loss = 1.9984334636022116 | Training Accuracy = 0.2\n",
      "Epoch 195/1000: Loss = 1.9928511114739318 | Training Accuracy = 0.2\n",
      "Epoch 200/1000: Loss = 1.9869781621528033 | Training Accuracy = 0.2\n",
      "Epoch 205/1000: Loss = 1.9808018301732502 | Training Accuracy = 0.2\n",
      "Epoch 210/1000: Loss = 1.974300052576126 | Training Accuracy = 0.2\n",
      "Epoch 215/1000: Loss = 1.9674795144405912 | Training Accuracy = 0.2\n",
      "Epoch 220/1000: Loss = 1.960284548244529 | Training Accuracy = 0.2\n",
      "Epoch 225/1000: Loss = 1.9527222528869106 | Training Accuracy = 0.2\n",
      "Epoch 230/1000: Loss = 1.9447547487394186 | Training Accuracy = 0.2\n",
      "Epoch 235/1000: Loss = 1.936385701898795 | Training Accuracy = 0.2\n",
      "Epoch 240/1000: Loss = 1.9275526912226961 | Training Accuracy = 0.2\n",
      "Epoch 245/1000: Loss = 1.9182821229101745 | Training Accuracy = 0.2\n",
      "Epoch 250/1000: Loss = 1.9085804598668474 | Training Accuracy = 0.2\n",
      "Epoch 255/1000: Loss = 1.8983715369511018 | Training Accuracy = 0.2\n",
      "Epoch 260/1000: Loss = 1.8876898691435442 | Training Accuracy = 0.2\n",
      "Epoch 265/1000: Loss = 1.8764755216101854 | Training Accuracy = 0.25\n",
      "Epoch 270/1000: Loss = 1.8647656389635479 | Training Accuracy = 0.25\n",
      "Epoch 275/1000: Loss = 1.8524871406308432 | Training Accuracy = 0.35\n",
      "Epoch 280/1000: Loss = 1.8397002311403354 | Training Accuracy = 0.35\n",
      "Epoch 285/1000: Loss = 1.8264458211630348 | Training Accuracy = 0.35\n",
      "Epoch 290/1000: Loss = 1.812675975931759 | Training Accuracy = 0.35\n",
      "Epoch 295/1000: Loss = 1.798365094439723 | Training Accuracy = 0.35\n",
      "Epoch 300/1000: Loss = 1.7836255586225402 | Training Accuracy = 0.35\n",
      "Epoch 305/1000: Loss = 1.768376359065103 | Training Accuracy = 0.35\n",
      "Epoch 310/1000: Loss = 1.7526203064163515 | Training Accuracy = 0.4\n",
      "Epoch 315/1000: Loss = 1.7363884675171513 | Training Accuracy = 0.45\n",
      "Epoch 320/1000: Loss = 1.7196565337755736 | Training Accuracy = 0.45\n",
      "Epoch 325/1000: Loss = 1.7025079169267587 | Training Accuracy = 0.55\n",
      "Epoch 330/1000: Loss = 1.684862136961415 | Training Accuracy = 0.55\n",
      "Epoch 335/1000: Loss = 1.6668900342877244 | Training Accuracy = 0.55\n",
      "Epoch 340/1000: Loss = 1.6484689845690632 | Training Accuracy = 0.55\n",
      "Epoch 345/1000: Loss = 1.6297196868357067 | Training Accuracy = 0.55\n",
      "Epoch 350/1000: Loss = 1.6107427428865848 | Training Accuracy = 0.55\n",
      "Epoch 355/1000: Loss = 1.5913619550639904 | Training Accuracy = 0.55\n",
      "Epoch 360/1000: Loss = 1.571849996588234 | Training Accuracy = 0.55\n",
      "Epoch 365/1000: Loss = 1.552003122404486 | Training Accuracy = 0.55\n",
      "Epoch 370/1000: Loss = 1.5319340451210013 | Training Accuracy = 0.55\n",
      "Epoch 375/1000: Loss = 1.5118252387866224 | Training Accuracy = 0.55\n",
      "Epoch 380/1000: Loss = 1.4915781119746143 | Training Accuracy = 0.6\n",
      "Epoch 385/1000: Loss = 1.4712402181356588 | Training Accuracy = 0.6\n",
      "Epoch 390/1000: Loss = 1.4507842356704634 | Training Accuracy = 0.6\n",
      "Epoch 395/1000: Loss = 1.4303775162338526 | Training Accuracy = 0.6\n",
      "Epoch 400/1000: Loss = 1.409943406824258 | Training Accuracy = 0.65\n",
      "Epoch 405/1000: Loss = 1.3895208333212465 | Training Accuracy = 0.65\n",
      "Epoch 410/1000: Loss = 1.3691289867597267 | Training Accuracy = 0.65\n",
      "Epoch 415/1000: Loss = 1.3488933881743816 | Training Accuracy = 0.65\n",
      "Epoch 420/1000: Loss = 1.3286985138111762 | Training Accuracy = 0.65\n",
      "Epoch 425/1000: Loss = 1.3086149389658635 | Training Accuracy = 0.7\n",
      "Epoch 430/1000: Loss = 1.288703258431894 | Training Accuracy = 0.7\n",
      "Epoch 435/1000: Loss = 1.2690566925942293 | Training Accuracy = 0.7\n",
      "Epoch 440/1000: Loss = 1.249427615792198 | Training Accuracy = 0.7\n",
      "Epoch 445/1000: Loss = 1.230130956163246 | Training Accuracy = 0.7\n",
      "Epoch 450/1000: Loss = 1.2109505368684101 | Training Accuracy = 0.7\n",
      "Epoch 455/1000: Loss = 1.1919107745371664 | Training Accuracy = 0.7\n",
      "Epoch 460/1000: Loss = 1.173151662518762 | Training Accuracy = 0.7\n",
      "Epoch 465/1000: Loss = 1.1546064074348528 | Training Accuracy = 0.7\n",
      "Epoch 470/1000: Loss = 1.1361846046768143 | Training Accuracy = 0.7\n",
      "Epoch 475/1000: Loss = 1.1180732559579174 | Training Accuracy = 0.7\n",
      "Epoch 480/1000: Loss = 1.100067131738594 | Training Accuracy = 0.7\n",
      "Epoch 485/1000: Loss = 1.0824749669918732 | Training Accuracy = 0.7\n",
      "Epoch 490/1000: Loss = 1.0649069679063008 | Training Accuracy = 0.7\n",
      "Epoch 495/1000: Loss = 1.0476197121561692 | Training Accuracy = 0.8\n",
      "Epoch 500/1000: Loss = 1.0306409500359868 | Training Accuracy = 0.85\n",
      "Epoch 505/1000: Loss = 1.01378121786953 | Training Accuracy = 0.85\n",
      "Epoch 510/1000: Loss = 0.9971962063273301 | Training Accuracy = 0.85\n",
      "Epoch 515/1000: Loss = 0.9808998397789648 | Training Accuracy = 0.85\n",
      "Epoch 520/1000: Loss = 0.964735934160354 | Training Accuracy = 0.85\n",
      "Epoch 525/1000: Loss = 0.9488403302065909 | Training Accuracy = 0.85\n",
      "Epoch 530/1000: Loss = 0.9331426302776163 | Training Accuracy = 0.85\n",
      "Epoch 535/1000: Loss = 0.9176908288275083 | Training Accuracy = 0.85\n",
      "Epoch 540/1000: Loss = 0.902415250512321 | Training Accuracy = 0.85\n",
      "Epoch 545/1000: Loss = 0.8873912610248649 | Training Accuracy = 0.9\n",
      "Epoch 550/1000: Loss = 0.8726631723738214 | Training Accuracy = 0.9\n",
      "Epoch 555/1000: Loss = 0.858072373101745 | Training Accuracy = 0.9\n",
      "Epoch 560/1000: Loss = 0.8436595497131656 | Training Accuracy = 0.9\n",
      "Epoch 565/1000: Loss = 0.8296138971228482 | Training Accuracy = 0.9\n",
      "Epoch 570/1000: Loss = 0.8156590276681598 | Training Accuracy = 0.9\n",
      "Epoch 575/1000: Loss = 0.8019492969473297 | Training Accuracy = 0.9\n",
      "Epoch 580/1000: Loss = 0.7884925242534127 | Training Accuracy = 0.95\n",
      "Epoch 585/1000: Loss = 0.775177081538081 | Training Accuracy = 0.95\n",
      "Epoch 590/1000: Loss = 0.7621752786623073 | Training Accuracy = 0.95\n",
      "Epoch 595/1000: Loss = 0.7492981492898521 | Training Accuracy = 0.95\n",
      "Epoch 600/1000: Loss = 0.7367315337841076 | Training Accuracy = 0.95\n",
      "Epoch 605/1000: Loss = 0.7243118963348156 | Training Accuracy = 0.95\n",
      "Epoch 610/1000: Loss = 0.7120999901237733 | Training Accuracy = 0.95\n",
      "Epoch 615/1000: Loss = 0.7001179044345466 | Training Accuracy = 0.95\n",
      "Epoch 620/1000: Loss = 0.6883674405695659 | Training Accuracy = 0.95\n",
      "Epoch 625/1000: Loss = 0.6767814867773041 | Training Accuracy = 0.95\n",
      "Epoch 630/1000: Loss = 0.6653876789473012 | Training Accuracy = 0.95\n",
      "Epoch 635/1000: Loss = 0.6542124790100469 | Training Accuracy = 0.95\n",
      "Epoch 640/1000: Loss = 0.6433018538391495 | Training Accuracy = 0.95\n",
      "Epoch 645/1000: Loss = 0.6325218357870547 | Training Accuracy = 0.95\n",
      "Epoch 650/1000: Loss = 0.6218872998536268 | Training Accuracy = 0.95\n",
      "Epoch 655/1000: Loss = 0.6114923978169153 | Training Accuracy = 0.95\n",
      "Epoch 660/1000: Loss = 0.601322748768891 | Training Accuracy = 0.95\n",
      "Epoch 665/1000: Loss = 0.591341509207354 | Training Accuracy = 0.95\n",
      "Epoch 670/1000: Loss = 0.5815322977870852 | Training Accuracy = 0.95\n",
      "Epoch 675/1000: Loss = 0.5718863691997084 | Training Accuracy = 0.95\n",
      "Epoch 680/1000: Loss = 0.5624295378996059 | Training Accuracy = 0.95\n",
      "Epoch 685/1000: Loss = 0.5531409709342289 | Training Accuracy = 0.95\n",
      "Epoch 690/1000: Loss = 0.5440561531482463 | Training Accuracy = 0.95\n",
      "Epoch 695/1000: Loss = 0.5351156183367064 | Training Accuracy = 0.95\n",
      "Epoch 700/1000: Loss = 0.5263653799814835 | Training Accuracy = 0.95\n",
      "Epoch 705/1000: Loss = 0.5177382049325118 | Training Accuracy = 0.95\n",
      "Epoch 710/1000: Loss = 0.5093008896680474 | Training Accuracy = 0.95\n",
      "Epoch 715/1000: Loss = 0.5010321200830867 | Training Accuracy = 0.95\n",
      "Epoch 720/1000: Loss = 0.4929673240629451 | Training Accuracy = 0.95\n",
      "Epoch 725/1000: Loss = 0.4849837800055382 | Training Accuracy = 0.95\n",
      "Epoch 730/1000: Loss = 0.47716753060257233 | Training Accuracy = 0.95\n",
      "Epoch 735/1000: Loss = 0.4695461770859791 | Training Accuracy = 0.95\n",
      "Epoch 740/1000: Loss = 0.46199547674442404 | Training Accuracy = 0.95\n",
      "Epoch 745/1000: Loss = 0.4546542134289697 | Training Accuracy = 0.95\n",
      "Epoch 750/1000: Loss = 0.44743506397965377 | Training Accuracy = 0.95\n",
      "Epoch 755/1000: Loss = 0.4403972580487747 | Training Accuracy = 0.95\n",
      "Epoch 760/1000: Loss = 0.43341287850998433 | Training Accuracy = 0.95\n",
      "Epoch 765/1000: Loss = 0.4266595286043725 | Training Accuracy = 0.95\n",
      "Epoch 770/1000: Loss = 0.4199575537627206 | Training Accuracy = 0.95\n",
      "Epoch 775/1000: Loss = 0.4134145750318769 | Training Accuracy = 0.95\n",
      "Epoch 780/1000: Loss = 0.40701544459014743 | Training Accuracy = 0.95\n",
      "Epoch 785/1000: Loss = 0.400715812214835 | Training Accuracy = 0.95\n",
      "Epoch 790/1000: Loss = 0.3945413054991797 | Training Accuracy = 0.95\n",
      "Epoch 795/1000: Loss = 0.38854306371002645 | Training Accuracy = 0.95\n",
      "Epoch 800/1000: Loss = 0.3825890831333515 | Training Accuracy = 0.95\n",
      "Epoch 805/1000: Loss = 0.37678854072267304 | Training Accuracy = 0.95\n",
      "Epoch 810/1000: Loss = 0.37111301842552785 | Training Accuracy = 0.95\n",
      "Epoch 815/1000: Loss = 0.36551606134215076 | Training Accuracy = 0.95\n",
      "Epoch 820/1000: Loss = 0.36001200622216134 | Training Accuracy = 0.95\n",
      "Epoch 825/1000: Loss = 0.3546632649519975 | Training Accuracy = 0.95\n",
      "Epoch 830/1000: Loss = 0.34937631240205835 | Training Accuracy = 0.95\n",
      "Epoch 835/1000: Loss = 0.34421960907473387 | Training Accuracy = 0.95\n",
      "Epoch 840/1000: Loss = 0.3391185153087611 | Training Accuracy = 0.95\n",
      "Epoch 845/1000: Loss = 0.33416301978570007 | Training Accuracy = 0.95\n",
      "Epoch 850/1000: Loss = 0.32932425577683716 | Training Accuracy = 0.95\n",
      "Epoch 855/1000: Loss = 0.32452514853438974 | Training Accuracy = 0.95\n",
      "Epoch 860/1000: Loss = 0.3198593682088804 | Training Accuracy = 0.95\n",
      "Epoch 865/1000: Loss = 0.3152844651631487 | Training Accuracy = 0.95\n",
      "Epoch 870/1000: Loss = 0.31075408521070846 | Training Accuracy = 0.95\n",
      "Epoch 875/1000: Loss = 0.3063505013457334 | Training Accuracy = 0.95\n",
      "Epoch 880/1000: Loss = 0.3020383397077391 | Training Accuracy = 0.95\n",
      "Epoch 885/1000: Loss = 0.29778241447009773 | Training Accuracy = 0.95\n",
      "Epoch 890/1000: Loss = 0.29359893552781646 | Training Accuracy = 0.95\n",
      "Epoch 895/1000: Loss = 0.28953825285395507 | Training Accuracy = 0.95\n",
      "Epoch 900/1000: Loss = 0.2855296376490946 | Training Accuracy = 0.95\n",
      "Epoch 905/1000: Loss = 0.2815802985273318 | Training Accuracy = 0.95\n",
      "Epoch 910/1000: Loss = 0.2777220299827162 | Training Accuracy = 0.95\n",
      "Epoch 915/1000: Loss = 0.2739254051386123 | Training Accuracy = 1.0\n",
      "Epoch 920/1000: Loss = 0.2702146371834119 | Training Accuracy = 1.0\n",
      "Epoch 925/1000: Loss = 0.26655931847278636 | Training Accuracy = 1.0\n",
      "Epoch 930/1000: Loss = 0.262967776561857 | Training Accuracy = 1.0\n",
      "Epoch 935/1000: Loss = 0.259496722953112 | Training Accuracy = 1.0\n",
      "Epoch 940/1000: Loss = 0.2560474804874541 | Training Accuracy = 1.0\n",
      "Epoch 945/1000: Loss = 0.2526373858482479 | Training Accuracy = 1.0\n",
      "Epoch 950/1000: Loss = 0.24930291956307235 | Training Accuracy = 1.0\n",
      "Epoch 955/1000: Loss = 0.2460597936714417 | Training Accuracy = 1.0\n",
      "Epoch 960/1000: Loss = 0.24286628032969731 | Training Accuracy = 1.0\n",
      "Epoch 965/1000: Loss = 0.2397139189043655 | Training Accuracy = 1.0\n",
      "Epoch 970/1000: Loss = 0.2366395427498415 | Training Accuracy = 1.0\n",
      "Epoch 975/1000: Loss = 0.23362970750061018 | Training Accuracy = 1.0\n",
      "Epoch 980/1000: Loss = 0.23063045372196522 | Training Accuracy = 1.0\n",
      "Epoch 985/1000: Loss = 0.2277048558487471 | Training Accuracy = 1.0\n",
      "Epoch 990/1000: Loss = 0.22484075957874392 | Training Accuracy = 1.0\n",
      "Epoch 995/1000: Loss = 0.22202207901889826 | Training Accuracy = 1.0\n",
      "CPU times: user 23min 3s, sys: 9min 28s, total: 32min 32s\n",
      "Wall time: 16min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = 0.02\n",
    "Lambda = 0\n",
    "train_and_test_loop(1000, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuRrWdOeTxV3"
   },
   "source": [
    "Very small loss, train accuracy going to 100, nice! We are successful in overfitting. If your accuracy is not 100%, then tweak the hyperparameters and epoch values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zxBEMQaMJwTY",
    "outputId": "5f97cb4c-e68b-4c42-8055-45634eaf6d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 1024) (42000,)\n",
      "Test set (18000, 1024) (18000,)\n"
     ]
    }
   ],
   "source": [
    "# Reload the original training and test set \n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the file as readonly\n",
    "h5f = h5py.File('/content/drive/My Drive/DLCP/Project-1/Data/SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "x_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "x_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1024)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1024)\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "X_val = x_test\n",
    "y_val = y_test\n",
    "\n",
    "print('Training set', x_train.shape, y_train.shape)\n",
    "print('Test set', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GC5Dr2mkT584"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DYsk3TkZcNX"
   },
   "source": [
    "###Step 4: Start with small regularization and find learning rate that makes the loss go down.\n",
    "\n",
    "- we start with Lambda(small regularization) = 1e-7\n",
    "- we start with a small learning rate =1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "ZZ_17DiyJwTc",
    "outputId": "fcce8ccf-c608-472c-ae65-427751a51444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100: Loss = 2.3129020523458985 | Training Accuracy = 0.1019047619047619\n",
      "Epoch 50/100: Loss = 2.3122106666718105 | Training Accuracy = 0.10164285714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09938888888888889"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the hyperparameters according to the above instructions\n",
    "lr = 1e-7\n",
    "Lambda = 1e-7\n",
    "iterations = 100\n",
    "#call the train and test function\n",
    "train_and_test_loop(iterations=iterations, lr=lr,Lambda=Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgZTVg__2iMP"
   },
   "source": [
    "### Step 5: Lets try a (larger) learning rate . \n",
    "\n",
    "- Learning rate lr  \n",
    "- Regularization lambda \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Ms0quIJ2JwTe",
    "outputId": "769660ab-cb43-480f-89dc-1f9b1eb8f047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 29.633920730298275 | Training Accuracy = 0.0999047619047619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10022222222222223"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Hyperparameters( High value for lr and low values for lambda)\n",
    "lr = 1\n",
    "Lambda = 0\n",
    "iterations = 1\n",
    "# Call the train and test function\n",
    "train_and_test_loop(iterations=iterations, lr=lr,Lambda=Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k657b4BL2xyW"
   },
   "source": [
    "**Observation** : Loss went up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRGWq8UmZcNg"
   },
   "source": [
    "### Step 6: Train the model for different learning rates (In a range) based on the learning from above steps\n",
    "\n",
    "- learning rate =\n",
    "- regularization remains the small, lambda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xMfH_C-XJwTh",
    "outputId": "3efd0a51-0f19-4687-b5e2-c14e59f25068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50: Loss = 2.3128706805053234 | Training Accuracy = 0.12461904761904762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2003888888888889"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "Lambda = 0.003\n",
    "train_and_test_loop(iterations=50, lr=lr,Lambda=Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C71RS8rnUBI-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-A9KC8sZcNk"
   },
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "### Cross validation Strategy\n",
    "\n",
    "\n",
    "- Do coarse -> fine cross-validation in stages\n",
    "\n",
    "- First stage: only a few epochs to get rough idea of what params work\n",
    "- Second stage: longer running time, finer search\n",
    "- … (repeat as necessary)\n",
    "\n",
    "### Tip for detecting explosions in the solver: \n",
    "- If the cost is ever > 3 * original cost, break out early\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4szkNo3KUKPC"
   },
   "source": [
    "### For example: Run coarse search for 10 times with different lr and Lambda values each with 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "tUxwf1VRUN4W",
    "outputId": "70e931da-7c19-4c52-bda6-9f18236a6ddb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in square\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/100: Best_val_acc: 0.10077777777777777, lr: 2.271906758517457, Lambda: 30903.229871518823\n",
      "\n",
      "Try 2/100: Best_val_acc: 0.10077777777777777, lr: 0.012953111407207103, Lambda: 2136.0687033782715\n",
      "\n",
      "Try 3/100: Best_val_acc: 0.10077777777777777, lr: 223.57309535727194, Lambda: 0.008505449308264152\n",
      "\n",
      "Try 4/100: Best_val_acc: 0.10066666666666667, lr: 3.3375363641427936e-07, Lambda: 20.03948678128411\n",
      "\n",
      "Try 5/100: Best_val_acc: 0.17333333333333334, lr: 0.00028048841273538885, Lambda: 0.00026288007569977496\n",
      "\n",
      "Try 6/100: Best_val_acc: 0.10033333333333333, lr: 1.91826859188316e-06, Lambda: 0.0001006377382343227\n",
      "\n",
      "Try 7/100: Best_val_acc: 0.10077777777777777, lr: 2256.444651199504, Lambda: 0.00016525793568113983\n",
      "\n",
      "Try 8/100: Best_val_acc: 0.10077777777777777, lr: 0.11385850756164226, Lambda: 12.70771186488584\n",
      "\n",
      "Try 9/100: Best_val_acc: 0.11683333333333333, lr: 0.10734762345471321, Lambda: 1.6164428866218095e-05\n",
      "\n",
      "CPU times: user 33min 40s, sys: 15min 11s, total: 48min 51s\n",
      "Wall time: 26min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run coarse search for a coarse range of lr and lambda values and print the results of the \n",
    "#first 10 epochs and figure out the range of lr and lambda for finer search\n",
    "import math\n",
    "import numpy as np\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-7.0, 4.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,5))\n",
    "    best_acc = train_and_test_loop(50, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTQlJqfEQeNA"
   },
   "source": [
    "\n",
    "**Learning rate and Lambda values corresponding to better accuracy rates**\n",
    "\n",
    "*   Try 5/100: Best_val_acc: 0.17333333333333334, lr: 0.00028048841273538885, Lambda: 0.00026288007569977496\n",
    "*   Try 9/100: Best_val_acc: 0.11683333333333333, lr: 0.10734762345471321, Lambda: 1.6164428866218095e-05\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78-Tf_-iUZbg"
   },
   "source": [
    "### Now run finer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "N2VYYC3kUhUL",
    "outputId": "10f5c798-8fc4-4055-89fb-3c04ae09f074"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in square\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in less_equal\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/10: Best_val_acc: [0.10077777777777777, nan], lr: 0.15387519821913492, Lambda: 0.00013496767005277493\n",
      "\n",
      "Try 2/10: Best_val_acc: [0.24983333333333332, 2.242363286315263], lr: 0.00664917999214434, Lambda: 0.21088513336418893\n",
      "\n",
      "Try 3/10: Best_val_acc: [0.0971111111111111, 2.3014919191907377], lr: 0.0016633161935173803, Lambda: 3.78245218499077e-05\n",
      "\n",
      "Try 4/10: Best_val_acc: [0.5913333333333334, 1.1222306557805626], lr: 0.04392805692170639, Lambda: 0.001266685087183563\n",
      "\n",
      "Try 5/10: Best_val_acc: [0.10077777777777777, 2.3546362981163472e+172], lr: 0.1572610007672735, Lambda: 2.1984793744019777\n",
      "\n",
      "Try 6/10: Best_val_acc: [0.10077777777777777, nan], lr: 3.7165775485436705, Lambda: 6.697891989596913\n",
      "\n",
      "Try 7/10: Best_val_acc: [0.6500555555555556, 1.0711399305850882], lr: 0.025595405777120297, Lambda: 1.050944460504173e-05\n",
      "\n",
      "Try 8/10: Best_val_acc: [0.10077777777777777, nan], lr: 0.35999478694745557, Lambda: 1.04700774838969e-05\n",
      "\n",
      "Try 9/10: Best_val_acc: [0.10077777777777777, 1.8158077792124403e+73], lr: 5.585100780017688, Lambda: 4.0623627724103485e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set a finer range of hyperparameters and figure out even finer range\n",
    "import math\n",
    "import numpy as np\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-3.0, 1.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,2))\n",
    "    best_acc = train_and_test_loop(10, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 10, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIoMoQOqND65"
   },
   "source": [
    "**Learning rate and Lambda values corresponding to better accuracy rates**\n",
    "\n",
    "\n",
    "*   Try 7/10: Best_val_acc: [0.6500555555555556, 1.0711399305850882], lr: 0.025595405777120297, Lambda: 1.050944460504173e-05\n",
    "*   Try 4/10: Best_val_acc: [0.5913333333333334, 1.1222306557805626], lr: 0.04392805692170639, Lambda: 0.001266685087183563\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7dkw-pvQb1n"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnzigYwKUnrL"
   },
   "source": [
    "### Running deep with the best possible lr and lambda and report the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3590
    },
    "colab_type": "code",
    "id": "3O5pYaNEUqmr",
    "outputId": "ee3f2f94-ea7e-4079-a641-9802c3ab8ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000: Loss = 2.3024304641468736 | Training Accuracy = 0.1079047619047619\n",
      "Epoch 5/1000: Loss = 1.8038414137294374 | Training Accuracy = 0.4488809523809524\n",
      "Epoch 10/1000: Loss = 1.2254019065202586 | Training Accuracy = 0.6408571428571429\n",
      "Epoch 15/1000: Loss = 1.0381807354321118 | Training Accuracy = 0.6973571428571429\n",
      "Epoch 20/1000: Loss = 0.9364721139791099 | Training Accuracy = 0.7283095238095239\n",
      "Epoch 25/1000: Loss = 0.8592783572683627 | Training Accuracy = 0.7375\n",
      "Epoch 30/1000: Loss = 0.7922491726768469 | Training Accuracy = 0.7585714285714286\n",
      "Epoch 35/1000: Loss = 0.7387019783120299 | Training Accuracy = 0.7800952380952381\n",
      "Epoch 40/1000: Loss = 0.7052297970016913 | Training Accuracy = 0.799\n",
      "Epoch 45/1000: Loss = 0.6761506174631248 | Training Accuracy = 0.8115714285714286\n",
      "Epoch 50/1000: Loss = 0.6483869543343672 | Training Accuracy = 0.8117619047619048\n",
      "Epoch 55/1000: Loss = 0.6291619966062538 | Training Accuracy = 0.8130952380952381\n",
      "Epoch 60/1000: Loss = 0.610811283341715 | Training Accuracy = 0.8256190476190476\n",
      "Epoch 65/1000: Loss = 0.5939839808652032 | Training Accuracy = 0.8284761904761905\n",
      "Epoch 70/1000: Loss = 0.5802008878922458 | Training Accuracy = 0.8344047619047619\n",
      "Epoch 75/1000: Loss = 0.5745424895463558 | Training Accuracy = 0.8252380952380952\n",
      "Epoch 80/1000: Loss = 0.5674802639968083 | Training Accuracy = 0.8302857142857143\n",
      "Epoch 85/1000: Loss = 0.5613490980269049 | Training Accuracy = 0.8244285714285714\n",
      "Epoch 90/1000: Loss = 0.5489915411199003 | Training Accuracy = 0.8354285714285714\n",
      "Epoch 95/1000: Loss = 0.5418078399475162 | Training Accuracy = 0.8228333333333333\n",
      "Epoch 100/1000: Loss = 0.5328308113511848 | Training Accuracy = 0.8268571428571428\n",
      "Epoch 105/1000: Loss = 0.535500778021088 | Training Accuracy = 0.8276904761904762\n",
      "Epoch 110/1000: Loss = 0.5238203481759939 | Training Accuracy = 0.8318809523809524\n",
      "Epoch 115/1000: Loss = 0.5232968825564247 | Training Accuracy = 0.8368809523809524\n",
      "Epoch 120/1000: Loss = 0.511172192299803 | Training Accuracy = 0.8467619047619047\n",
      "Epoch 125/1000: Loss = 0.5072716018405456 | Training Accuracy = 0.8440714285714286\n",
      "Epoch 130/1000: Loss = 0.49522224162532813 | Training Accuracy = 0.8461666666666666\n",
      "Epoch 135/1000: Loss = 0.48820731438906534 | Training Accuracy = 0.8443095238095238\n",
      "Epoch 140/1000: Loss = 0.4762300308914751 | Training Accuracy = 0.8528095238095238\n",
      "Epoch 145/1000: Loss = 0.4748244266243452 | Training Accuracy = 0.8449761904761904\n",
      "Epoch 150/1000: Loss = 0.47034557351563144 | Training Accuracy = 0.8425\n",
      "Epoch 155/1000: Loss = 0.4630147107968378 | Training Accuracy = 0.8415714285714285\n",
      "Epoch 160/1000: Loss = 0.455551789538642 | Training Accuracy = 0.8447857142857143\n",
      "Epoch 165/1000: Loss = 0.4485523707388383 | Training Accuracy = 0.8465952380952381\n",
      "Epoch 170/1000: Loss = 0.4410080884171539 | Training Accuracy = 0.8576666666666667\n",
      "Epoch 175/1000: Loss = 0.43524755879061167 | Training Accuracy = 0.8547142857142858\n",
      "Epoch 180/1000: Loss = 0.4293182254755374 | Training Accuracy = 0.8598095238095238\n",
      "Epoch 185/1000: Loss = 0.4267060160484655 | Training Accuracy = 0.8617857142857143\n",
      "Epoch 190/1000: Loss = 0.42786664533880736 | Training Accuracy = 0.8577380952380952\n",
      "Epoch 195/1000: Loss = 0.42806035846520474 | Training Accuracy = 0.855547619047619\n",
      "Epoch 200/1000: Loss = 0.4276884554599442 | Training Accuracy = 0.8516904761904762\n",
      "Epoch 205/1000: Loss = 0.4285117725335952 | Training Accuracy = 0.8493571428571428\n",
      "Epoch 210/1000: Loss = 0.4228031281909682 | Training Accuracy = 0.8497619047619047\n",
      "Epoch 215/1000: Loss = 0.4215218930074582 | Training Accuracy = 0.8505952380952381\n",
      "Epoch 220/1000: Loss = 0.42041533394239566 | Training Accuracy = 0.8509761904761904\n",
      "Epoch 225/1000: Loss = 0.41556248029179677 | Training Accuracy = 0.852047619047619\n",
      "Epoch 230/1000: Loss = 0.4079718446739847 | Training Accuracy = 0.8524047619047619\n",
      "Epoch 235/1000: Loss = 0.4119310171583447 | Training Accuracy = 0.8497142857142858\n",
      "Epoch 240/1000: Loss = 0.41066673775915596 | Training Accuracy = 0.8491190476190477\n",
      "Epoch 245/1000: Loss = 0.4145047340103612 | Training Accuracy = 0.8473095238095238\n",
      "Epoch 250/1000: Loss = 0.41612330310816015 | Training Accuracy = 0.8475714285714285\n",
      "Epoch 255/1000: Loss = 0.40384544080393975 | Training Accuracy = 0.8624047619047619\n",
      "Epoch 260/1000: Loss = 0.393116846407072 | Training Accuracy = 0.8620714285714286\n",
      "Epoch 265/1000: Loss = 0.38369733181982224 | Training Accuracy = 0.8620714285714286\n",
      "Epoch 270/1000: Loss = 0.3770630205824611 | Training Accuracy = 0.8647619047619047\n",
      "Epoch 275/1000: Loss = 0.37162795994414005 | Training Accuracy = 0.8671428571428571\n",
      "Epoch 280/1000: Loss = 0.3669141447486463 | Training Accuracy = 0.8683095238095239\n",
      "Epoch 285/1000: Loss = 0.36494654007270744 | Training Accuracy = 0.8677380952380952\n",
      "Epoch 290/1000: Loss = 0.3625940928239485 | Training Accuracy = 0.8707857142857143\n",
      "Epoch 295/1000: Loss = 0.3598487312660078 | Training Accuracy = 0.8710238095238095\n",
      "Epoch 300/1000: Loss = 0.3582005594217875 | Training Accuracy = 0.8732619047619048\n",
      "Epoch 305/1000: Loss = 0.35740698415730116 | Training Accuracy = 0.871904761904762\n",
      "Epoch 310/1000: Loss = 0.35591603852011006 | Training Accuracy = 0.8737619047619047\n",
      "Epoch 315/1000: Loss = 0.35584642020157053 | Training Accuracy = 0.8751666666666666\n",
      "Epoch 320/1000: Loss = 0.35349895171247697 | Training Accuracy = 0.8727857142857143\n",
      "Epoch 325/1000: Loss = 0.35507783235361146 | Training Accuracy = 0.8739047619047619\n",
      "Epoch 330/1000: Loss = 0.3543205950055981 | Training Accuracy = 0.8754285714285714\n",
      "Epoch 335/1000: Loss = 0.35860586072448664 | Training Accuracy = 0.8720238095238095\n",
      "Epoch 340/1000: Loss = 0.3578601889134252 | Training Accuracy = 0.8718095238095238\n",
      "Epoch 345/1000: Loss = 0.3527984761089636 | Training Accuracy = 0.873452380952381\n",
      "Epoch 350/1000: Loss = 0.35065838761795554 | Training Accuracy = 0.8713571428571428\n",
      "Epoch 355/1000: Loss = 0.34972818241642867 | Training Accuracy = 0.8716666666666667\n",
      "Epoch 360/1000: Loss = 0.34991873461626666 | Training Accuracy = 0.8703095238095238\n",
      "Epoch 365/1000: Loss = 0.35276362585295346 | Training Accuracy = 0.8747857142857143\n",
      "Epoch 370/1000: Loss = 0.3471749961011313 | Training Accuracy = 0.8763333333333333\n",
      "Epoch 375/1000: Loss = 0.34431826261443 | Training Accuracy = 0.878095238095238\n",
      "Epoch 380/1000: Loss = 0.3448071453139261 | Training Accuracy = 0.8766904761904762\n",
      "Epoch 385/1000: Loss = 0.3474733517319946 | Training Accuracy = 0.8752380952380953\n",
      "Epoch 390/1000: Loss = 0.3462022728739021 | Training Accuracy = 0.8737619047619047\n",
      "Epoch 395/1000: Loss = 0.3440517031313096 | Training Accuracy = 0.8745714285714286\n",
      "Epoch 400/1000: Loss = 0.34337328982968224 | Training Accuracy = 0.8736904761904762\n",
      "Epoch 405/1000: Loss = 0.34701357672647376 | Training Accuracy = 0.8732142857142857\n",
      "Epoch 410/1000: Loss = 0.3455737756130995 | Training Accuracy = 0.8770476190476191\n",
      "Epoch 415/1000: Loss = 0.34331362825353895 | Training Accuracy = 0.8765\n",
      "Epoch 420/1000: Loss = 0.3424251407159595 | Training Accuracy = 0.8811904761904762\n",
      "Epoch 425/1000: Loss = 0.34188343419791317 | Training Accuracy = 0.8828095238095238\n",
      "Epoch 430/1000: Loss = 0.3369681418632728 | Training Accuracy = 0.8883809523809524\n",
      "Epoch 435/1000: Loss = 0.33750337992606133 | Training Accuracy = 0.8892619047619048\n",
      "Epoch 440/1000: Loss = 0.3355108070453059 | Training Accuracy = 0.8883095238095238\n",
      "Epoch 445/1000: Loss = 0.3334111279754118 | Training Accuracy = 0.8875\n",
      "Epoch 450/1000: Loss = 0.32611171932567695 | Training Accuracy = 0.8870714285714286\n",
      "Epoch 455/1000: Loss = 0.3250837940943304 | Training Accuracy = 0.8896428571428572\n",
      "Epoch 460/1000: Loss = 0.3207947751138919 | Training Accuracy = 0.8912857142857142\n",
      "Epoch 465/1000: Loss = 0.3161279532527183 | Training Accuracy = 0.8893333333333333\n",
      "Epoch 470/1000: Loss = 0.3186119276474726 | Training Accuracy = 0.8874761904761905\n",
      "Epoch 475/1000: Loss = 0.31900542615447675 | Training Accuracy = 0.8885238095238095\n",
      "Epoch 480/1000: Loss = 0.32332152186658614 | Training Accuracy = 0.8842380952380953\n",
      "Epoch 485/1000: Loss = 0.31893044476342847 | Training Accuracy = 0.8885\n",
      "Epoch 490/1000: Loss = 0.32315022597193116 | Training Accuracy = 0.8840238095238095\n",
      "Epoch 495/1000: Loss = 0.32134828713622826 | Training Accuracy = 0.8846428571428572\n",
      "Epoch 500/1000: Loss = 0.32177382673413857 | Training Accuracy = 0.8811904761904762\n",
      "Epoch 505/1000: Loss = 0.32139280250823693 | Training Accuracy = 0.8844047619047619\n",
      "Epoch 510/1000: Loss = 0.3199729382973209 | Training Accuracy = 0.8823571428571428\n",
      "Epoch 515/1000: Loss = 0.31870179352458394 | Training Accuracy = 0.8853809523809524\n",
      "Epoch 520/1000: Loss = 0.32015855939073495 | Training Accuracy = 0.8790476190476191\n",
      "Epoch 525/1000: Loss = 0.3241180275515362 | Training Accuracy = 0.8770952380952381\n",
      "Epoch 530/1000: Loss = 0.32300845734761524 | Training Accuracy = 0.877547619047619\n",
      "Epoch 535/1000: Loss = 0.32674027370829484 | Training Accuracy = 0.8746666666666667\n",
      "Epoch 540/1000: Loss = 0.327796765083929 | Training Accuracy = 0.8730238095238095\n",
      "Epoch 545/1000: Loss = 0.32639768525707674 | Training Accuracy = 0.8856428571428572\n",
      "Epoch 550/1000: Loss = 0.33004750590770526 | Training Accuracy = 0.8877857142857143\n",
      "Epoch 555/1000: Loss = 0.32860190910073955 | Training Accuracy = 0.885452380952381\n",
      "Epoch 560/1000: Loss = 0.3288062770141761 | Training Accuracy = 0.8840238095238095\n",
      "Epoch 565/1000: Loss = 0.32684660072271193 | Training Accuracy = 0.8893095238095238\n",
      "Epoch 570/1000: Loss = 0.3206014544993673 | Training Accuracy = 0.8876190476190476\n",
      "Epoch 575/1000: Loss = 0.3157781539520503 | Training Accuracy = 0.8908333333333334\n",
      "Epoch 580/1000: Loss = 0.3103993110311501 | Training Accuracy = 0.8901190476190476\n",
      "Epoch 585/1000: Loss = 0.3051410699218994 | Training Accuracy = 0.8920238095238096\n",
      "Epoch 590/1000: Loss = 0.30158156887787607 | Training Accuracy = 0.8931190476190476\n",
      "Epoch 595/1000: Loss = 0.30002110880339006 | Training Accuracy = 0.8889523809523809\n",
      "Epoch 600/1000: Loss = 0.29796382831781226 | Training Accuracy = 0.8896428571428572\n",
      "Epoch 605/1000: Loss = 0.30004880752575 | Training Accuracy = 0.8882857142857142\n",
      "Epoch 610/1000: Loss = 0.2955315132595498 | Training Accuracy = 0.888547619047619\n",
      "Epoch 615/1000: Loss = 0.29540167851955945 | Training Accuracy = 0.885547619047619\n",
      "Epoch 620/1000: Loss = 0.2941917147722764 | Training Accuracy = 0.8864761904761905\n",
      "Epoch 625/1000: Loss = 0.2951294532698514 | Training Accuracy = 0.8827857142857143\n",
      "Epoch 630/1000: Loss = 0.2922327997419305 | Training Accuracy = 0.8856666666666667\n",
      "Epoch 635/1000: Loss = 0.2922625149400113 | Training Accuracy = 0.8874047619047619\n",
      "Epoch 640/1000: Loss = 0.2908924755377992 | Training Accuracy = 0.8826428571428572\n",
      "Epoch 645/1000: Loss = 0.2896764173777775 | Training Accuracy = 0.8886666666666667\n",
      "Epoch 650/1000: Loss = 0.2892270367239904 | Training Accuracy = 0.8988809523809523\n",
      "Epoch 655/1000: Loss = 0.2912759639439414 | Training Accuracy = 0.9006904761904762\n",
      "Epoch 660/1000: Loss = 0.29381515131818214 | Training Accuracy = 0.9007857142857143\n",
      "Epoch 665/1000: Loss = 0.29181150142316836 | Training Accuracy = 0.9020952380952381\n",
      "Epoch 670/1000: Loss = 0.29572842801180477 | Training Accuracy = 0.8999285714285714\n",
      "Epoch 675/1000: Loss = 0.29393638776086123 | Training Accuracy = 0.9004761904761904\n",
      "Epoch 680/1000: Loss = 0.28995761278659526 | Training Accuracy = 0.9010714285714285\n",
      "Epoch 685/1000: Loss = 0.29048523089603073 | Training Accuracy = 0.8933571428571428\n",
      "Epoch 690/1000: Loss = 0.28517272980878583 | Training Accuracy = 0.8952857142857142\n",
      "Epoch 695/1000: Loss = 0.2857323901381714 | Training Accuracy = 0.8954047619047619\n",
      "Epoch 700/1000: Loss = 0.2826301250463505 | Training Accuracy = 0.9017380952380952\n",
      "Epoch 705/1000: Loss = 0.2822008178784439 | Training Accuracy = 0.9017619047619048\n",
      "Epoch 710/1000: Loss = 0.281710294357481 | Training Accuracy = 0.8997619047619048\n",
      "Epoch 715/1000: Loss = 0.27859584315850094 | Training Accuracy = 0.9006190476190477\n",
      "Epoch 720/1000: Loss = 0.2771115131403676 | Training Accuracy = 0.9007380952380952\n",
      "Epoch 725/1000: Loss = 0.2760127814128846 | Training Accuracy = 0.8988809523809523\n",
      "Epoch 730/1000: Loss = 0.27255472549539617 | Training Accuracy = 0.9013333333333333\n",
      "Epoch 735/1000: Loss = 0.27117673010049187 | Training Accuracy = 0.8987380952380952\n",
      "Epoch 740/1000: Loss = 0.2736282802099247 | Training Accuracy = 0.8966666666666666\n",
      "Epoch 745/1000: Loss = 0.2742399302728035 | Training Accuracy = 0.8950714285714285\n",
      "Epoch 750/1000: Loss = 0.2730005031418951 | Training Accuracy = 0.8966666666666666\n",
      "Epoch 755/1000: Loss = 0.2718996111035606 | Training Accuracy = 0.8985952380952381\n",
      "Epoch 760/1000: Loss = 0.2724965326466577 | Training Accuracy = 0.9013571428571429\n",
      "Epoch 765/1000: Loss = 0.2729064422062462 | Training Accuracy = 0.9030238095238096\n",
      "Epoch 770/1000: Loss = 0.2734621176931544 | Training Accuracy = 0.8995238095238095\n",
      "Epoch 775/1000: Loss = 0.27434202243185274 | Training Accuracy = 0.9030476190476191\n",
      "Epoch 780/1000: Loss = 0.27754509807165084 | Training Accuracy = 0.904547619047619\n",
      "Epoch 785/1000: Loss = 0.2794635040745578 | Training Accuracy = 0.9023809523809524\n",
      "Epoch 790/1000: Loss = 0.28294942988698685 | Training Accuracy = 0.9043571428571429\n",
      "Epoch 795/1000: Loss = 0.28510603685066793 | Training Accuracy = 0.9071904761904762\n",
      "Epoch 800/1000: Loss = 0.2894192528869521 | Training Accuracy = 0.9031666666666667\n",
      "Epoch 805/1000: Loss = 0.29318555610020103 | Training Accuracy = 0.9013571428571429\n",
      "Epoch 810/1000: Loss = 0.29445191249121655 | Training Accuracy = 0.902452380952381\n",
      "Epoch 815/1000: Loss = 0.2982019716760671 | Training Accuracy = 0.9012619047619047\n",
      "Epoch 820/1000: Loss = 0.29954206990869325 | Training Accuracy = 0.8969761904761905\n",
      "Epoch 825/1000: Loss = 0.3049105167723403 | Training Accuracy = 0.8955714285714286\n",
      "Epoch 830/1000: Loss = 0.3050592182620001 | Training Accuracy = 0.8951666666666667\n",
      "Epoch 835/1000: Loss = 0.3099224772252289 | Training Accuracy = 0.8936428571428572\n",
      "Epoch 840/1000: Loss = 0.31419798455407866 | Training Accuracy = 0.8967857142857143\n",
      "Epoch 845/1000: Loss = 0.3212518830095236 | Training Accuracy = 0.8941904761904762\n",
      "Epoch 850/1000: Loss = 0.31750084481723145 | Training Accuracy = 0.8958809523809523\n",
      "Epoch 855/1000: Loss = 0.3152108944609279 | Training Accuracy = 0.8999761904761905\n",
      "Epoch 860/1000: Loss = 0.31421690861523094 | Training Accuracy = 0.9007380952380952\n",
      "Epoch 865/1000: Loss = 0.3200817835329613 | Training Accuracy = 0.8831904761904762\n",
      "Epoch 870/1000: Loss = 0.32990562821588115 | Training Accuracy = 0.8666190476190476\n",
      "Epoch 875/1000: Loss = 0.3304183733117948 | Training Accuracy = 0.8718571428571429\n",
      "Epoch 880/1000: Loss = 0.3260745332625403 | Training Accuracy = 0.8763095238095238\n",
      "Epoch 885/1000: Loss = 0.32254601885354267 | Training Accuracy = 0.8712142857142857\n",
      "Epoch 890/1000: Loss = 0.3208777015361088 | Training Accuracy = 0.8663809523809524\n",
      "Epoch 895/1000: Loss = 0.3119928611473342 | Training Accuracy = 0.8744047619047619\n",
      "Epoch 900/1000: Loss = 0.3194467133185443 | Training Accuracy = 0.8878333333333334\n",
      "Epoch 905/1000: Loss = 0.3291043634832183 | Training Accuracy = 0.8841428571428571\n",
      "Epoch 910/1000: Loss = 0.3259142884049331 | Training Accuracy = 0.8735\n",
      "Epoch 915/1000: Loss = 0.3197963820304781 | Training Accuracy = 0.8795\n",
      "Epoch 920/1000: Loss = 0.3167106530662865 | Training Accuracy = 0.877547619047619\n",
      "Epoch 925/1000: Loss = 0.30669022487295505 | Training Accuracy = 0.8763333333333333\n",
      "Epoch 930/1000: Loss = 0.29514851430764927 | Training Accuracy = 0.8849047619047619\n",
      "Epoch 935/1000: Loss = 0.29234157725644655 | Training Accuracy = 0.8893333333333333\n",
      "Epoch 940/1000: Loss = 0.28622636542916524 | Training Accuracy = 0.8962619047619048\n",
      "Epoch 945/1000: Loss = 0.2822438454886487 | Training Accuracy = 0.8984047619047619\n",
      "Epoch 950/1000: Loss = 0.2787179550323016 | Training Accuracy = 0.8993571428571429\n",
      "Epoch 955/1000: Loss = 0.27763942266871844 | Training Accuracy = 0.8999761904761905\n",
      "Epoch 960/1000: Loss = 0.27905680101771074 | Training Accuracy = 0.898452380952381\n",
      "Epoch 965/1000: Loss = 0.27992564956331684 | Training Accuracy = 0.8974285714285715\n",
      "Epoch 970/1000: Loss = 0.2808226495184887 | Training Accuracy = 0.8917857142857143\n",
      "Epoch 975/1000: Loss = 0.2826657288271138 | Training Accuracy = 0.8971428571428571\n",
      "Epoch 980/1000: Loss = 0.2814214809805231 | Training Accuracy = 0.9009285714285714\n",
      "Epoch 985/1000: Loss = 0.2803509499404173 | Training Accuracy = 0.8990238095238096\n",
      "Epoch 990/1000: Loss = 0.28100520717032096 | Training Accuracy = 0.9014761904761904\n",
      "Epoch 995/1000: Loss = 0.2856234952280039 | Training Accuracy = 0.8964047619047619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.823"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set the best hyperparameters found in the previous steps\n",
    "lr =  0.02\n",
    "Lambda =  0.0003\n",
    "iterations = 1000\n",
    "#Call the train and test function (with score)\n",
    "train_and_test_loop(iterations, lr, Lambda)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AnupriyaRamachandran_DLCP_Project1_Milestone3.2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
